from arkham.Bayes.Quantify.BERT import BERT, tokenize
from arkham.Bayes.Quantify.data import get_tokenizer
import transformers
import tensorflow as tf

import numpy as np
from tqdm import tqdm


def test_NER_tokenizer():
    """
    https://github.com/huggingface/transformers/issues/64
    """
    sentence = "John Johanson lives in Ramat Gan."
    labels = ['B-PERS', 'I-PERS', 'O', 'B-LOC', 'I-LOC']  # @word-level
    labels2id = {'B-PERS': 0, 'I-PERS': 1, 'O': 2, 'B-LOC': 3, 'I-LOC': 4}
    sent = ['[CLS]', 'john', 'johan', '##son', 'lives', 'in', 'ramat', 'gan', '.', '[SEP]']
    labels = [2, 0, 1, 1, 2, 2, 3, 4, 2, 2]
    wordpiece_labels = ['O', 'B-PERS', 'I-PERS', 'I-PERS', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O']
    id2labels = {v: k for k, v in labels2id.items()}
    # [id2labels[l] for l in labels]
    attention_mask = [0, 1, 1, 1, 1, 1, 1, 1, 1, 0]
    sentence_id = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

    model_class = "distilbert-base-uncased"
    # sequence_labels
    tokenizer = get_tokenizer(model_class)

    # assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)
    tokenized_input = tokenizer(sentence, truncation=True)  # is_split_into_words=False/True if pretokenized

    print(tokenized_input)
    tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
    print(tokens)

    res = tokenize(
        [sentence],
        tokenizer,
        max_document_len=20,
        sequence_labels=True,
        labels=[labels],
        label_all_tokens=True,
        is_split_into_words=False,
    )
    print(res)

    res = tokenize(
        [sentence],
        tokenizer,
        max_document_len=20,
        sequence_labels=True,
        labels=[labels],
        label_all_tokens=False,
        is_split_into_words=False,
    )
    print(res)


def sparse_crossentropy_masked(y_true, y_pred):
    mask_value = -100
    y_true_masked = tf.boolean_mask(y_true, tf.not_equal(y_true, mask_value))
    y_pred_masked = tf.boolean_mask(y_pred, tf.not_equal(y_true, mask_value))
    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true_masked, y_pred_masked))


def categorical_crossentropy_masked(y_true, y_pred):
    mask_value = -100
    y_true_masked = tf.boolean_mask(y_true, tf.reduce_any(tf.not_equal(y_true, mask_value), 1))
    y_pred_masked = tf.boolean_mask(y_pred, tf.reduce_any(tf.not_equal(y_true, mask_value), 1))
    return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true_masked, y_pred_masked))


def test_masked():
    # logits = [[4.3, -0.5, -2.7, 0, 0], [0.5, 2.3, 0, 0, 0]]
    # labels = [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0]]

    y_true = tf.constant(np.array([0.0, 1.0, 2.0, -100]))
    y_pred = tf.constant(np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]))
    loss_op = sparse_crossentropy_masked(y_true, y_pred)
    print(loss_op)

    y_true_1 = tf.constant(np.array([0.0, 1.0, 2.0]))
    y_pred_1 = tf.constant(np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]))
    loss_1_op = sparse_crossentropy_masked(y_true_1, y_pred_1)
    print(loss_1_op)

    assert loss_op == loss_1_op

    y_true = tf.constant(np.array([0.0, 1.0, 2.0, 1]))
    y_pred = tf.constant(np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]))
    loss_op = sparse_crossentropy_masked(y_true, y_pred)
    print(loss_op)

    # mask = tf.equal(logits, 0) # as in the OP
    # weights =  tf.cast(mask, dtype=tf.float32) # convert to (0, 1) weights
    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels, weights=weights))


def test_mask_acc():
    from arkham.utils.losses import sparse_categorical_accuracy_masked

    labels = np.array([0.0, 1.0, 2.0, -100])
    example = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])
    batch_labels = np.array([labels, labels])
    batch_examples = np.array([example, example])

    y_true = tf.constant(batch_labels)
    y_pred = tf.constant(batch_examples)
    loss_op = sparse_categorical_accuracy_masked(y_true, y_pred)
    print(loss_op)

    # y_true_1 = tf.constant(np.array([0.,1.,2.]))
    # y_pred_1 = tf.constant(np.array([[1.,0.,0.], [0.,1.,0.], [0.,0.,1.]]))
    # loss_1_op = sparse_categorical_accuracy_masked(y_true_1, y_pred_1)
    # print(loss_1_op)

    # assert loss_op == loss_1_op

    # y_true = tf.constant(np.array([0.,1.,2., 1]))
    # y_pred = tf.constant(np.array([[1.,0.,0.], [0.,1.,0.], [0.,0.,1.], [0.,0.,1.]]))
    # loss_op = sparse_categorical_accuracy_masked(y_true, y_pred)
    # print(loss_op)


def test_dynamic_mask():
    def most_frequent(l):
        return max(set(l), key=l.count)

    def dynamic_mask(array, mask):
        """
        inputted array has to be np.array
        should still add checks on size :) 
        """
        new_array = []
        for indices in mask:
            if isinstance(indices, list):
                if not indices:  # masking pad, cls, and sep
                    continue
                elif isinstance(array[indices[0]], str):
                    new_array.append(most_frequent(array[indices].tolist()))  # VOTE most frequent
                else:
                    new_array.append(np.mean(array[indices], axis=-1))  # AVERAGE
            else:
                new_array.append(array[indices])
        return np.array(new_array)

    wordpieces = [
        '[CLS]',
        'CR',
        '##IC',
        '##KE',
        '##T',
        '-',
        'L',
        '##EI',
        '##CE',
        '##ST',
        '##ER',
        '##S',
        '##H',
        '##IR',
        '##E',
        'T',
        '##A',
        '##KE',
        'O',
        '##VE',
        '##R',
        'AT',
        'TO',
        '##P',
        'A',
        '##FT',
        '##ER',
        'IN',
        '##NI',
        '##NG',
        '##S',
        'VI',
        '##CT',
        '##OR',
        '##Y',
        '.',
        '[SEP]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
        '[PAD]',
    ]
    labels = [
        None,
        'O',
        'O',
        'O',
        'O',
        'O',
        'B-ORG',
        'B-ORG',
        'B-ORG',
        'B-ORG',
        'B-ORG',
        'B-ORG',
        'B-ORG',
        'B-ORG',
        'B-ORG',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        'O',
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
        None,
    ]
    inputs = np.array(
        [
            101,
            15531,
            9741,
            22441,
            1942,
            118,
            149,
            27514,
            10954,
            9272,
            9637,
            1708,
            3048,
            18172,
            2036,
            157,
            1592,
            22441,
            152,
            17145,
            2069,
            13020,
            16972,
            2101,
            138,
            26321,
            9637,
            15969,
            27451,
            11780,
            1708,
            7118,
            16647,
            9565,
            3663,
            119,
            102,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
        ],
        dtype=np.int32,
    )

    mask = [
        i for i, t in enumerate(wordpieces) if not (t.startswith("##") or t in ['[CLS]', '[SEP]', '[PAD]'])
    ]  # if first wordpiece tactic = OK
    group_level_indices = []
    current = []
    for i, wp in enumerate(reversed(wordpieces)):
        i = abs(i - len(wordpieces) + 1)
        if wp.startswith("##"):
            current.append(i)
        else:
            if wp in ['[CLS]', '[SEP]', '[PAD]']:
                group_level_indices.append([])
                continue

            if current:
                current.append(i)
                current = sorted(current)
                group_level_indices.append(current)
                current = []
            else:
                group_level_indices.append(i)

    group_level_indices = list(reversed(group_level_indices))
    print(group_level_indices)

    dynamic_labels = dynamic_mask(np.array(labels), np.array(group_level_indices, dtype="object"))
    return dynamic_labels, group_level_indices


def test_dynamic_wordpiece_3D():
    from arkham.Bayes.Quantify.evaluate import dynamic_wordpiece_mask
    import pickle

    array = pickle.load(open("/home/jordy/code/gordon/arkham/arkham/Bayes/Quantify/example.pickle", "rb"))
    mask = pickle.load(open("/home/jordy/code/gordon/arkham/arkham/Bayes/Quantify/examplemask.pickle", "rb"))

    threeD_arr = dynamic_wordpiece_mask(array, mask)
    print(threeD_arr.shape)
    # print(threeD_arr)

    twoD_arr = dynamic_wordpiece_mask(array[0, :, :], mask)
    print(twoD_arr.shape)

    oneD_arr = dynamic_wordpiece_mask(array[0, :, 0], mask)
    print(oneD_arr.shape)
    # print(twoD_arr)
    # new_array = []
    # for indices in mask:
    #     if isinstance(indices,list):
    #         if not indices:
    #             continue
    #         masked = np.mean(array[:,indices,:], axis=1)
    #         new_array.append(masked)
    #     else:
    #         new_array.append(array[:,indices,:])

    # new_array = np.array(new_array)
    # new_array = np.transpose(new_array, (1,0,2))
    # return new_array
